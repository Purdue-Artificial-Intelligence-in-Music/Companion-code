{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "MIDI-DDSP Demo",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "<a href=\"https://colab.research.google.com/github/magenta/midi-ddsp/blob/main/midi_ddsp/colab/MIDI_DDSP_Demo.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>\n",
        "\n",
        "##### Copyright 2022 The MIDI-DDSP Authors.\n",
        "\n",
        "Licensed under the Apache License, Version 2.0 (the \"License\");\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "3HWdNocrJzgY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#@title\n",
        "#  Copyright 2022 The MIDI-DDSP Authors.\n",
        "#  #\n",
        "#  Licensed under the Apache License, Version 2.0 (the \"License\");\n",
        "#  you may not use this file except in compliance with the License.\n",
        "#  You may obtain a copy of the License at\n",
        "#  #\n",
        "#      http://www.apache.org/licenses/LICENSE-2.0\n",
        "#  #\n",
        "#  Unless required by applicable law or agreed to in writing, software\n",
        "#  distributed under the License is distributed on an \"AS IS\" BASIS,\n",
        "#  WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
        "#  See the License for the specific language governing permissions and\n",
        "#  limitations under the License."
      ],
      "metadata": {
        "cellView": "form",
        "id": "ZBy5NF8iJ3q7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aZUBBozs6haz"
      },
      "source": [
        "# MIDI-DDSP Demo\n",
        "Here is the demo where you can automatically synthesize MIDI with the proposed model and then edit the note expression controls of each note. This can be seen as a prototype of our system where users can interact with the model and create the desired music audio together.\n",
        "\n",
        "[MIDI-DDSP ICLR paper]()\n",
        "\n",
        "[Audio Examples]()\n",
        "\n",
        "<img src=\"https://midi-ddsp.github.io/pics/midi-ddsp-diagram-hori.png\" alt=\"MIDI-DDSP\" width=\"700\">\n",
        "\n",
        "### Instructions for running:\n",
        "\n",
        "* Make sure to use a GPU runtime, click:  __Runtime >> Change Runtime Type >> GPU__\n",
        "* Press ▶️ on the left of each of the cells\n",
        "* View the code: Double-click any of the cells\n",
        "* Hide the code: Double click the right side of the cell\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dO4EmHiR3H70",
        "cellView": "form"
      },
      "source": [
        "#@title #Install Dependencies, Import Code and Setup Models\n",
        "\n",
        "#@markdown Run this cell to install dependencies, import codes, \n",
        "#@markdown setup utility functions and load MIDI-DDSP model weights.\n",
        "#@markdown Running this cell could take a while.\n",
        "!pip install -q git+https://github.com/lukewys/qgrid.git\n",
        "\n",
        "!pip install -q git+https://github.com/magenta/midi-ddsp\n",
        "!midi_ddsp_download_model_weights\n",
        "\n",
        "!git clone -q https://github.com/magenta/midi-ddsp.git\n",
        "!wget -q https://keymusician01.s3.amazonaws.com/FluidR3_GM.zip\n",
        "!unzip -q FluidR3_GM.zip\n",
        "\n",
        "# Ignore a bunch of deprecation warnings\n",
        "import sys\n",
        "sys.path.append('./midi-ddsp')\n",
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\")\n",
        "\n",
        "import os\n",
        "import librosa\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import tensorflow.compat.v2 as tf\n",
        "import pandas as pd\n",
        "import qgrid\n",
        "import music21\n",
        "from IPython.display import Javascript\n",
        "import IPython.display as ipd\n",
        "from google.colab import files\n",
        "from google.colab import output\n",
        "output.enable_custom_widget_manager()\n",
        "\n",
        "from midi_ddsp import load_pretrained_model\n",
        "from midi_ddsp.utils.training_utils import set_seed, get_hp\n",
        "from midi_ddsp.hparams_synthesis_generator import hparams as hp\n",
        "from midi_ddsp.modules.get_synthesis_generator import get_synthesis_generator, get_fake_data_synthesis_generator\n",
        "from midi_ddsp.modules.expression_generator import ExpressionGenerator, get_fake_data_expression_generator\n",
        "from midi_ddsp.utils.audio_io import save_wav\n",
        "from midi_ddsp.utils.midi_synthesis_utils import synthesize_mono_midi, synthesize_bach\n",
        "from midi_ddsp.midi_ddsp_synthesize import synthesize_midi\n",
        "from midi_ddsp.utils.inference_utils import conditioning_df_to_audio, get_process_group\n",
        "from midi_ddsp.data_handling.instrument_name_utils import INST_NAME_TO_ID_DICT, INST_NAME_LIST\n",
        "\n",
        "set_seed(1234)\n",
        "sample_rate = 16000\n",
        "\n",
        "synthesis_generator, expression_generator = load_pretrained_model()\n",
        "\n",
        "def plot_spec(wav, sr, title='', play=True, vmin=-8, vmax=1, save_path=None):\n",
        "    D = np.log(np.abs(librosa.stft(wav, n_fft=512 + 256)))\n",
        "    librosa.display.specshow(D, sr=sr, vmin=vmin, vmax=vmax, cmap='magma')\n",
        "    plt.title(title)\n",
        "    wav = np.clip(wav, -1, 1)\n",
        "    if play:\n",
        "        ipd.display(ipd.Audio(wav, rate=sr))\n",
        "    if save_path:\n",
        "        plt.savefig(save_path)\n",
        "        plt.close()\n",
        "\n",
        "EDIT_DF_NAME_ORDER = ['volume', 'vol_fluc', 'vol_peak_pos', 'vibrato', 'brightness', 'attack', 'pitch', 'note_length']\n",
        "\n",
        "COND_DF_NAME_ORDER = ['volume', 'vol_fluc', 'vibrato', 'brightness', 'attack', 'vol_peak_pos', 'pitch', 'onset', 'offset', 'note_length']\n",
        "\n",
        "def conditioning_df_to_edit_df(conditioning_df):\n",
        "  edit_df = conditioning_df.copy()\n",
        "  return edit_df[EDIT_DF_NAME_ORDER]\n",
        "\n",
        "def edit_df_to_conditioning_df(edit_df):\n",
        "  conditioning_df = edit_df.copy()\n",
        "  note_length = conditioning_df['note_length'].values\n",
        "  offset = np.cumsum(note_length)\n",
        "  onset = np.concatenate([[0],offset[:-1]])\n",
        "  conditioning_df['onset']=onset\n",
        "  conditioning_df['offset']=offset\n",
        "  return conditioning_df[COND_DF_NAME_ORDER]\n",
        "\n",
        "GAIN_ADJUST_DB_DICT = {    \n",
        "    'string_set': {    \n",
        "        'Soprano': 2,\n",
        "        'Alto': 2,\n",
        "        'Tenor': -1,\n",
        "        'Bass': -1,\n",
        "    },\n",
        "    'woodwind_set': {    \n",
        "        'Soprano': 1.5,\n",
        "        'Alto': 1.2,\n",
        "        'Tenor': 0,\n",
        "        'Bass': 1.8,\n",
        "    },\n",
        "    'brasswind_set': {    \n",
        "        'Soprano': 2,\n",
        "        'Alto': 2,\n",
        "        'Tenor': 5.6,\n",
        "        'Bass': 2.9,\n",
        "    },\n",
        "\n",
        "}\n",
        "\n",
        "def upload_midi():\n",
        "  midi_files = files.upload()\n",
        "  fnames = list(midi_files.keys())\n",
        "  return fnames\n",
        "\n",
        "print('Done!')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CVRDl2QBf71C"
      },
      "source": [
        "## Monophonic MIDI Synthesis"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "cellView": "form",
        "id": "qkINRV9Tk8BT"
      },
      "source": [
        "#@markdown Let's first synthesize a MIDI using MIDI-DDSP! By running this cell without any change, MIDI-DDSP will synthesis the MIDI of \"ode to joy\" using violin. This will take about a minute.\n",
        "\n",
        "#@markdown You can also upload your own MIDI file for MIDI-DDSP to synthesize! (by changing the `midi_file` to \"Upload (.mid)\")\n",
        "#@markdown There are 13 instruments available. You can select instrument in the dropdown menu next to \"instrument\".\n",
        "#@markdown Besides changing instruments and MIDI file, there are two variables you could change to adjust the MIDI synthesis:\n",
        "#@markdown - `pitch_offset`: transpose the MIDI file for `pitch_offset` semitones. (>0 is pitch up, <0 is pitch down). \n",
        "#@markdown Different instrument has different pitch range. Please consider adjusting the `pitch_offset` for different instrument depending on the MIDI file.\n",
        "#@markdown - `speed_rate`: adjust play speed of the MIDI (=1: original speed, >1: faster, <1: slower).\n",
        "\n",
        "#@markdown In this cell, we will only synthesizing a single monophonic track. If you upload multi-track MIDI, only the first track will be used. You can synthesize multi-track MIDI using MIDI-DDSP in the cell below.\n",
        "\n",
        "#@markdown The generation speed would be 2.5x-5x realtime. That is, one need to wait 24-50 seconds for render a 10 second MIDI.\n",
        "\n",
        "midi_file = 'Ode to Joy' #@param ['Ode to Joy','Upload (.mid)']\n",
        "\n",
        "instrument = \"violin\"  #@param ['violin', 'viola', 'cello', 'double bass', 'flute', 'oboe', 'clarinet', 'saxophone', 'bassoon', 'trumpet', 'horn', 'trombone', 'tuba']\n",
        "\n",
        "pitch_offset =  0#@param {type:\"integer\"}\n",
        "\n",
        "speed_rate =  1#@param {type:\"number\", min:0}\n",
        "\n",
        "if midi_file == 'Ode to Joy':\n",
        "  midi_file = r'./midi-ddsp/midi_example/ode_to_joy.mid'\n",
        "else:\n",
        "  midi_file = upload_midi()[0]\n",
        "\n",
        "instrument_name = instrument\n",
        "\n",
        "instrument_id = INST_NAME_TO_ID_DICT[instrument_name]\n",
        "midi_audio, midi_control_params, midi_synth_params, conditioning_df = synthesize_mono_midi(synthesis_generator, expression_generator, midi_file, instrument_id, output_dir=None, pitch_offset=pitch_offset, speed_rate=speed_rate)\n",
        "plt.figure(figsize=(15,5))\n",
        "plot_spec(midi_audio[0].numpy(), sr=16000)\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pVZ0hinyiUp7"
      },
      "source": [
        "## Adjusting Note Expression"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XfPPrdPu5sSy"
      },
      "source": [
        "In MIDI-DDSP, six note expression controls are designed and used to control the expressive performance, all in range of [0,1]. You can adjust them to edit the performance aspect of the MIDI synthesis:\n",
        " - Volume (`volume`): Controls overall volume of a note. (larger value -> larger volume)\n",
        " - Volume Fluctuation (`vol_fluc`): Controls the extent of the volume changing in a note (crescendo & decrescendo or not). (larger value -> more extensive dynamic changing)\n",
        " - Volume Peak Position (`vol_peak_pos`): Controls the volume changing in a note (crescendo & decrescendo, together with amplitude_std). (larger value -> later reach maximum volume)\n",
        " - Attack Noise (`attack_noise`): Controls the extent of note attack (strong or soft). (larger value -> larger attack)\n",
        " - Brightness (`brightness`): Controls the timbre of a note. (larger value -> brighter / more amplitude on higher harmonics)\n",
        " - Vibrato (`vibrato`): Controls the extend of the vibrato of a note. (larger value -> larger vibrato extend)\n",
        "\n",
        " "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5q2DmNEO3H8J",
        "cellView": "form"
      },
      "source": [
        "#@markdown Run this cell to get an editable table for adjusting note expression.\n",
        "#@markdown The values shown are predicted by note expression control generator.\n",
        "#@markdown Each row is a note. Double click the item in the table and enter the value, click anywhere else to save the change. \n",
        "\n",
        "#@markdown You can also edit the note pitch and length just as editing a MIDI sequence.\n",
        "#@markdown To add a note or remove a note, click \"Add Row\" and \"Remove Row\" on the upper left corner.\n",
        "\n",
        "#@markdown After edit, the table is changed automatically and you can run the synthesize cell below the next cell to synthesize the result. \n",
        "#@markdown **Run this cell again will reset the table to the initial value.**\n",
        "\n",
        "qgrid_widget = qgrid.show_grid(conditioning_df_to_edit_df(conditioning_df), show_toolbar=True)\n",
        "qgrid_widget"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "cellView": "form",
        "id": "kBAjQYSW3H8K",
        "scrolled": true
      },
      "source": [
        "#@markdown Run this cell to synthesize with edited note expression controls.\n",
        "\n",
        "conditioning_df_changed = edit_df_to_conditioning_df(qgrid_widget.get_changed_df())\n",
        "midi_audio, midi_control_params, midi_synth_params = conditioning_df_to_audio(synthesis_generator, conditioning_df_changed, tf.constant([instrument_id]), display_progressbar=True)\n",
        "plt.figure(figsize=(15,5))\n",
        "plot_spec(midi_audio[0].numpy(), sr=16000)\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YWs-peNlsilU"
      },
      "source": [
        "## Bach Chorales Synthesis"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "cellView": "form",
        "id": "6PD7OeeKEYkX"
      },
      "source": [
        "#@markdown Besides synthesizing monophonic MIDI, MIDI-DDSP can also synthesizing multi-track MIDI.\n",
        "#@markdown Running this cell to synthesis quartet of [4-part Bach Chorales](https://en.wikipedia.org/wiki/List_of_chorale_harmonisations_by_Johann_Sebastian_Bach).\n",
        "\n",
        "#@markdown You can synthesize any Bach Chorales by typing in piece number to `piece_number` below. A full list of bach chorales available can be found [here](https://github.com/cuthbertLab/music21/tree/master/music21/corpus/bach).\n",
        "\n",
        "#@markdown We provide three quartet settings, string, woodwind and brass wind. You can change the ensemble by selecting from the drop down menu of `ensemble`.\n",
        "\n",
        "\n",
        "piece_number = 'bwv227.1' #@param {type:\"string\"}\n",
        "\n",
        "ensemble = 'string_set' #@param ['string_set', 'woodwind_set', 'brasswind_set']\n",
        "\n",
        "score = music21.corpus.parse(f'bach/{piece_number}')\n",
        "score.write('midi', fp=f'./{piece_number}.mid')\n",
        "midi_file = f'./{piece_number}.mid'\n",
        "midi_audio_mix, midi_audio_all, midi_control_params, midi_synth_params, conditioning_df_all = synthesize_bach(\n",
        "    synthesis_generator, \n",
        "    expression_generator, \n",
        "    midi_file, \n",
        "    quartet_set=ensemble, \n",
        "    pitch_offset=0, \n",
        "    speed_rate=1, \n",
        "    output_dir='./', \n",
        "    gain_adjust_db_dict=GAIN_ADJUST_DB_DICT[ensemble])\n",
        "\n",
        "part_name = list(GAIN_ADJUST_DB_DICT[ensemble].keys())\n",
        "plt.figure(figsize=(15,5))\n",
        "plot_spec(midi_audio_mix, sr=16000, title='Mix')\n",
        "plt.show()\n",
        "\n",
        "for i in range(len(midi_audio_all)):\n",
        "    plt.figure(figsize=(15,5))\n",
        "    plot_spec(midi_audio_all[i], sr=16000, title=part_name[i])\n",
        "    plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "g0fzbo7Nz-sF"
      },
      "source": [
        "## Pitch Bend by Editing Synthesis Parameters"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "cellView": "form",
        "id": "bEpDheCtEnl4"
      },
      "source": [
        "#@markdown Run this cell to generate a pitch bend by editing synthesis parameters.\n",
        "#@markdown You can double-click the cell to see how we do it in the code.\n",
        "#@markdown This is just an example. \n",
        "#@markdown We encourage you to come up with smarter ways to smooth the connection and \n",
        "#@markdown crazy ways to play with the synthesis parameters :).\n",
        "\n",
        "# First define functions to generate pitch bend. \n",
        "# You can come up with your own way of edit pitch or\n",
        "# other synthesis parameters.\n",
        "\n",
        "def get_pitch_bend(start_value, end_value, length, power=4,offset_1=-1,bend_type='exp'):\n",
        "    if bend_type=='exp':\n",
        "        if start_value <= end_value:\n",
        "            value = start_value + (np.power(np.linspace(0.0, 1.0, num=length)+offset_1, power)-offset_1) * (end_value-start_value)\n",
        "        return value\n",
        "    elif bend_type=='linear':\n",
        "        return np.linspace(start_value, end_value, length)\n",
        "\n",
        "# First run the MIDI-DDSP to get the synthesis parameters predicted\n",
        "# You don't need to run this if you already have the synthesis parameter prediction.\n",
        "midi_file = r'./midi-ddsp/midi_example/ode_to_joy.mid'\n",
        "instrument_name = 'violin'\n",
        "instrument_id = INST_NAME_TO_ID_DICT[instrument_name]\n",
        "midi_audio, midi_control_params, midi_synth_params, conditioning_df = synthesize_mono_midi(synthesis_generator, expression_generator, midi_file, instrument_id, output_dir=None, pitch_offset=pitch_offset, speed_rate=speed_rate)\n",
        "\n",
        "# Assume we want to add a pitch bend in the middle of two notes. \n",
        "# The first note ends at frame 368 while the next note start at the frame 375\n",
        "\n",
        "prev_note_off = 368\n",
        "next_note_on = 375\n",
        "f0_ori = midi_synth_params['f0_hz'][0,...,0]\n",
        "amps_ori = midi_synth_params['amplitudes'].numpy()[0,...,0]\n",
        "noise_ori = midi_synth_params['noise_magnitudes'].numpy()\n",
        "hd_ori = midi_synth_params['harmonic_distribution'].numpy()\n",
        "\n",
        "# Edit the f0 to add the pitch bend, starting from \n",
        "# 20 frames before the previous note off to 50 frames after next note on.\n",
        "edit_frame_start = prev_note_off-20\n",
        "edit_frame_end = next_note_on+50\n",
        "edit_frame_duration = edit_frame_end - edit_frame_start\n",
        "\n",
        "f0_changed = tf.concat([f0_ori[:edit_frame_start], \n",
        "            get_pitch_bend(f0_ori[edit_frame_start], \n",
        "                    f0_ori[edit_frame_end], \n",
        "                    (edit_frame_end)-(edit_frame_start), \n",
        "                    power=7, \n",
        "                    bend_type='exp'), \n",
        "            f0_ori[edit_frame_end:]], axis=0)\n",
        "f0_changed = f0_changed[tf.newaxis, ..., tf.newaxis]\n",
        "\n",
        "# For other synthesis parameters, use that from the start of the next note \n",
        "# to replace the connection of notes. \n",
        "# We also need to avoid the onset of the next note, \n",
        "# thus that is the reason we use \"next_note_on+5\" as the start.\n",
        "amps_changed = amps_ori\n",
        "amps_changed[edit_frame_start:edit_frame_end] = amps_ori[next_note_on+5:next_note_on+edit_frame_duration+5]\n",
        "amps_changed = amps_changed[tf.newaxis, ..., tf.newaxis]\n",
        "noise_changed = noise_ori\n",
        "noise_changed[0,edit_frame_start:edit_frame_end,:] = noise_ori[0,next_note_on+5:next_note_on+edit_frame_duration+5,:]\n",
        "hd_changed = hd_ori\n",
        "hd_changed[0,edit_frame_start:edit_frame_end,:] = hd_changed[0,next_note_on+5:next_note_on+edit_frame_duration+5,:]\n",
        "\n",
        "# Resynthesis the audio using DDSP\n",
        "processor_group = get_process_group(midi_synth_params['amplitudes'].shape[1], use_angular_cumsum=True)\n",
        "midi_audio_changed = processor_group({'amplitudes': amps_changed,\n",
        "                        'harmonic_distribution': hd_changed,\n",
        "                        'noise_magnitudes': noise_changed,\n",
        "                        'f0_hz': f0_changed,},\n",
        "                          verbose=False)\n",
        "if synthesis_generator.reverb_module is not None:\n",
        "    midi_audio_changed = synthesis_generator.reverb_module(midi_audio_changed, reverb_number=instrument_id, training=False)\n",
        "\n",
        "plt.figure(figsize=(15,5))\n",
        "# Just play the first 4 seconds\n",
        "plot_spec(midi_audio[0].numpy()[:4*16000], sr=16000, title='Original')\n",
        "plt.show()\n",
        "plt.figure(figsize=(15,5))\n",
        "plot_spec(midi_audio_changed[0].numpy()[:4*16000], sr=16000, title='Add pitch bend')\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5xYX-txJ1B13"
      },
      "source": [
        "## Multi-track MIDI Synthesis"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "cellView": "form",
        "id": "IpddFjD97QoV"
      },
      "source": [
        "#@markdown Run this cell to upload and synthesize any multi-track MIDI.\n",
        "\n",
        "#@markdown For midi programs that are not supported by MIDI-DDSP, we will use [FluidSynth](https://www.fluidsynth.org/) to synthesize the track.\n",
        "#@markdown For midi programs supported by MIDI-DDSP, it will only synthesize a monophonic performance. \n",
        "#@markdown That is, for a polyphonic track, only a polyphonic note sequence will be synthesized.\n",
        "\n",
        "pitch_offset =  0#@param {type:\"integer\"}\n",
        "\n",
        "speed_rate =  1#@param {type:\"number\", min:0}\n",
        "\n",
        "midi_file = upload_midi()[0]\n",
        "\n",
        "output = synthesize_midi(synthesis_generator, expression_generator, midi_file,\n",
        "              pitch_offset=pitch_offset, speed_rate=speed_rate,\n",
        "              output_dir=r'./',\n",
        "              use_fluidsynth=True,\n",
        "              sf2_path='./FluidR3_GM.sf2',\n",
        "              display_progressbar=True)\n",
        "\n",
        "plot_spec(output['midi_audio_mix'], sr=16000, title='Mix')\n",
        "\n",
        "for i in range(len(output['stem_audio'])):\n",
        "    plt.figure(figsize=(15,5))\n",
        "    plot_spec(output['stem_audio'][i], sr=16000, title=f'Track {i}')\n",
        "    plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}